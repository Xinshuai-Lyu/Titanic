{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "Vjs3Am7NTz2q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwHVVgte_CHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da45c998-953c-4ac7-fe73-3eb4d805a1b0"
      },
      "source": [
        "!pip install --user kaggle\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/\"\n",
        "!kaggle competitions download -c titanic\n",
        "%pip install -q tensorflow-recommenders\n",
        "import tensorflow_recommenders as tfrs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "SEED=2021"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/59.8k [00:00<?, ?B/s]\n",
            "100% 59.8k/59.8k [00:00<00:00, 56.8MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/28.0k [00:00<?, ?B/s]\n",
            "100% 28.0k/28.0k [00:00<00:00, 50.5MB/s]\n",
            "Downloading gender_submission.csv to /content\n",
            "  0% 0.00/3.18k [00:00<?, ?B/s]\n",
            "100% 3.18k/3.18k [00:00<00:00, 3.38MB/s]\n",
            "\u001b[K     |████████████████████████████████| 85 kB 4.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-wsxNi9Mgz1"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### General helpers"
      ],
      "metadata": {
        "id": "Kl1RxAzcqrQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(df):\n",
        "  df = df.sample(frac=1, random_state=SEED)\n",
        "  df = df.reset_index(drop=True)\n",
        "  return df\n",
        "def normalize_df(df):\n",
        "  columns = df.columns\n",
        "  for c in columns:\n",
        "    df[c] = (df[c] - np.min(df[c])) / (np.max(df[c]) - np.min(df[c]))\n",
        "  return df\n",
        "def _split_train_valid_(X, y, frac):\n",
        "  n = int(len(X)*frac)\n",
        "  train_X, train_y = X[0:n], y[0:n] \n",
        "  valid_X, valid_y = X[n:], y[n:]\n",
        "  return (train_X, train_y, valid_X, valid_y)\n",
        "def split_train_valid(df, y_column, X_columns=None, frac=0.85):\n",
        "  if X_columns:\n",
        "    X = df[X_columns].to_numpy()\n",
        "  else:\n",
        "    X = df.drop(columns=[y_column]).to_numpy()\n",
        "  y = df[y_column].to_numpy()\n",
        "  return _split_train_valid_(X, y, frac)\n",
        "def series_map(series, f):\n",
        "  return series.apply(f)"
      ],
      "metadata": {
        "id": "6q1a7U06qtrI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model related helpers"
      ],
      "metadata": {
        "id": "zmxn3yh6rupZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class vocab_lookup_layer(keras.layers.Layer):\n",
        "    # https://stackoverflow.com/questions/58507400/how-to-use-tf-lookup-tables-with-tensorflow-2-0-keras-and-mlflow\n",
        "    def __init__(self, vocab, num_oov_buckets, **kwargs):\n",
        "      self.vocab = vocab\n",
        "      self.num_oov_buckets = num_oov_buckets\n",
        "      super(vocab_lookup_layer, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "      vocab_initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "        self.vocab, tf.range(len(self.vocab), dtype=tf.int64)\n",
        "      )\n",
        "      self.table = tf.lookup.StaticVocabularyTable(vocab_initializer, self.num_oov_buckets)\n",
        "      self.built = True\n",
        "    def call(self, inputs):\n",
        "      return self.table.lookup(inputs)\n",
        "    def get_config(self):\n",
        "      return {'vocab': self.vocab, 'num_oov_buckets': self.num_oov_buckets}\n",
        "def embedding_layer(vocab, output_dim=4, num_oov_buckets=100, dtype=tf.int64):\n",
        "    table = vocab_lookup_layer(vocab, num_oov_buckets)\n",
        "    categorical_feature_input = keras.layers.Input(shape=[], dtype=dtype)\n",
        "    indexes = keras.layers.Lambda(lambda c: table(c))(categorical_feature_input)\n",
        "    embeddings = keras.layers.Embedding(input_dim=len(vocab)+num_oov_buckets, output_dim=output_dim)(indexes)\n",
        "    return categorical_feature_input, embeddings\n",
        "def train_model(model, X, y, valid_X, valid_y, loss, \n",
        "                callbacks_flag=True, \n",
        "                epochs=100000,\n",
        "                metrics=[tf.keras.metrics.PrecisionAtRecall(0.85), \n",
        "                         keras.metrics.binary_accuracy],\n",
        "                monitor=\"val_binary_accuracy\", \n",
        "                class_weight={},\n",
        "                lr=1e-3,\n",
        "                patience=20, \n",
        "                decay_rate=0.1):\n",
        "  if callbacks_flag:\n",
        "    callbacks = [\n",
        "      keras.callbacks.EarlyStopping(\n",
        "        patience=patience, \n",
        "        restore_best_weights=True, \n",
        "        monitor=monitor\n",
        "      )\n",
        "    ]\n",
        "  else:\n",
        "    callbacks = []\n",
        "  lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=lr,\n",
        "    decay_steps=500,\n",
        "    decay_rate=decay_rate)\n",
        "  optimizer=keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "  model.compile(\n",
        "      optimizer=optimizer, \n",
        "      loss=loss,\n",
        "      metrics=metrics\n",
        "  )\n",
        "  hist = model.fit(\n",
        "      x=X, \n",
        "      y=y, \n",
        "      validation_data=(valid_X, valid_y), \n",
        "      epochs=epochs,\n",
        "      callbacks=callbacks,\n",
        "      class_weight=class_weight\n",
        "  ) \n",
        "  return hist\n",
        "def dense_layer(n_units, previous_output, BN=True, DR=False,\n",
        "                KR=None, name=None, activation=keras.activations.relu,\n",
        "                KI='he_normal'):\n",
        "    layer = keras.layers.Dense(\n",
        "        n_units, \n",
        "        activation=activation, \n",
        "        kernel_initializer=KI,\n",
        "        kernel_regularizer=KR\n",
        "    )\n",
        "    output = layer(previous_output)\n",
        "    if name:\n",
        "        layer._name=name\n",
        "    if DR:\n",
        "        output = keras.layers.Dropout(DR)(output)\n",
        "    if BN:\n",
        "        output = keras.layers.BatchNormalization()(output)\n",
        "    return output\n",
        "def get_the_best_epochs_num(\n",
        "    hist, \n",
        "    best_epoch_metric=\"val_binary_accuracy\", \n",
        "    metrics=[\"val_precision_at_recall\", \"val_binary_accuracy\"]\n",
        "):\n",
        "  n_epochs_best = np.argmax(hist.history[best_epoch_metric])\n",
        "  results = [n_epochs_best+1, hist.history[\"val_loss\"][n_epochs_best]]\n",
        "  for metric in metrics:\n",
        "    results.append(hist.history[metric][n_epochs_best])\n",
        "  return results\n",
        "def get_model_without_top(model):\n",
        "  model_reduced = keras.Sequential()\n",
        "  for layer in model.layers[:-1]:\n",
        "    model_reduced.add(layer)\n",
        "  return model_reduced"
      ],
      "metadata": {
        "id": "W5DTq_FXrwyt"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Titanic helpers"
      ],
      "metadata": {
        "id": "uN_8zg_Xqwo6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_5eYC70MesK"
      },
      "source": [
        "def save_submission(predictions):\n",
        "  to_1_or_0 = (predictions >= 0.5)\n",
        "  to_int = np.array(to_1_or_0, dtype=int)\n",
        "  survivied_predictions = to_int.flatten()\n",
        "  submission = pd.DataFrame(data={\"PassengerId\":PassengerId,\n",
        "  \"Survived\":survivied_predictions})\n",
        "  submission.to_csv(\"submission_df\", index=False)\n",
        "def get_df_numpy(df):\n",
        "  if type(df) == tuple:\n",
        "    df_numpy = df\n",
        "  else:\n",
        "    df_numpy = df.to_numpy()\n",
        "  return df_numpy\n",
        "def submit(df, model):\n",
        "  predictions = model.predict(get_df_numpy(df))\n",
        "  save_submission(predictions)\n",
        "def ensemble_submit(models):\n",
        "  flag = False\n",
        "  evaluation_results = []\n",
        "  for model, df, evaluation_result in models:\n",
        "    if not flag:\n",
        "      predictions = np.ndarray.flatten(model.predict(get_df_numpy(df)))*evaluation_result\n",
        "      flag = True\n",
        "    else:\n",
        "      predictions = np.add(predictions, np.ndarray.flatten(model.predict(get_df_numpy(df)))*evaluation_result)\n",
        "    evaluation_results.append(evaluation_result)\n",
        "  predictions = predictions/np.sum(evaluation_results)\n",
        "  save_submission(predictions)\n",
        "  return predictions"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqEgqI6MavZ"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original dataset"
      ],
      "metadata": {
        "id": "Pg_hGwID40uF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciRHBKECMaLr"
      },
      "source": [
        "titanic_train = pd.read_csv(\"/content/train.csv\")\n",
        "titanic_test = pd.read_csv(\"/content/test.csv\")\n",
        "PassengerId = titanic_test.PassengerId"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Exploration"
      ],
      "metadata": {
        "id": "QkNaPmxE7KFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_train.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "GCwD4sLy7RYe",
        "outputId": "2a620e65-3e08-4e33-ef20-a6fa5457f73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>714.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.383838</td>\n",
              "      <td>2.308642</td>\n",
              "      <td>29.699118</td>\n",
              "      <td>0.523008</td>\n",
              "      <td>0.381594</td>\n",
              "      <td>32.204208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>257.353842</td>\n",
              "      <td>0.486592</td>\n",
              "      <td>0.836071</td>\n",
              "      <td>14.526497</td>\n",
              "      <td>1.102743</td>\n",
              "      <td>0.806057</td>\n",
              "      <td>49.693429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>223.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>20.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.910400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.454200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>668.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>512.329200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\n",
              "count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\n",
              "mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\n",
              "std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\n",
              "min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n",
              "25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n",
              "50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n",
              "75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\n",
              "max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=titanic_train, x=\"Age\", hue=\"Survived\", stat=\"density\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "wnp9ZGrU9_qP",
        "outputId": "a7de8ee6-400e-4a4b-8ad4-a6c0bea9ce94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f93205b64d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tV4SQH_GyXi"
      },
      "source": [
        "### data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ta4htNPac2jz"
      },
      "source": [
        "def base_preprocess(df):\n",
        "  df = df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"])\n",
        "  df = df.dropna(subset=[\"Embarked\"])\n",
        "  df[\"Sex\"] = df.Sex.map({\"male\":1, \"female\":0})\n",
        "  df[\"Embarked\"] = df.Embarked.map({\"S\":0, \"C\":1, \"Q\":2})\n",
        "  return df\n",
        "def base_preprocess_2(df):\n",
        "  df = df.drop(columns=[\"PassengerId\", \"Ticket\", \"Cabin\"])\n",
        "  df = df.dropna(subset=[\"Embarked\"])\n",
        "  df[\"Sex\"] = df.Sex.map({\"male\":1, \"female\":0})\n",
        "  df[\"Embarked\"] = df.Embarked.map({\"S\":0, \"C\":1, \"Q\":2})\n",
        "  return df\n",
        "def cut_age(df):\n",
        "  bins = pd.IntervalIndex.from_tuples([(0, 10), (10, 70), (70, 1000)])\n",
        "  df[\"Age\"] = pd.cut(df.Age, bins=bins).cat.codes.astype(int)\n",
        "  return df\n",
        "def cut_sibsp(df):\n",
        "  bins = pd.IntervalIndex.from_tuples([(-1, 0), (0, 10000)])\n",
        "  df[\"SibSp\"] = pd.cut(df.SibSp, bins=bins).cat.codes.astype(int)\n",
        "  return df\n",
        "def cut_parch(df):\n",
        "  bins = pd.IntervalIndex.from_tuples([(-1, 0), (0, 10000)])\n",
        "  df[\"Parch\"] = pd.cut(df.Parch, bins=bins).cat.codes.astype(int)\n",
        "  return df\n",
        "def cut_fare(df):\n",
        "  bins = pd.IntervalIndex.from_tuples([(-1, 8), (8, 15), (15, 31), (31, 10000)])\n",
        "  df[\"Fare\"] = pd.cut(df.Fare, bins=bins).cat.codes.astype(int)\n",
        "  return df\n",
        "def hot_encoding(df, age=True):\n",
        "  if age:\n",
        "    data = [\n",
        "      df,\n",
        "      pd.get_dummies(df['Embarked'], prefix='Embarked', dtype=np.int8),\n",
        "      pd.get_dummies(df['Sex'], prefix='Sex', dtype=np.int8),\n",
        "      pd.get_dummies(df['Pclass'], prefix='Pclass', dtype=np.int8),\n",
        "      pd.get_dummies(df['Age'], prefix='Pclass', dtype=np.int8),\n",
        "      pd.get_dummies(df['SibSp'], prefix='SibSp', dtype=np.int8),\n",
        "      pd.get_dummies(df['Parch'], prefix='Parch', dtype=np.int8),\n",
        "      pd.get_dummies(df['Fare'], prefix='Fare', dtype=np.int8)        \n",
        "    ]\n",
        "    columns=[\"Pclass\", \"Sex\", \"Embarked\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "  else:\n",
        "    data = [\n",
        "      df,\n",
        "      pd.get_dummies(df['Embarked'], prefix='Embarked', dtype=np.int8),\n",
        "      pd.get_dummies(df['Sex'], prefix='Sex', dtype=np.int8),\n",
        "      pd.get_dummies(df['Pclass'], prefix='Pclass', dtype=np.int8),\n",
        "      pd.get_dummies(df['SibSp'], prefix='SibSp', dtype=np.int8),\n",
        "      pd.get_dummies(df['Parch'], prefix='Parch', dtype=np.int8),\n",
        "      pd.get_dummies(df['Fare'], prefix='Fare', dtype=np.int8)        \n",
        "    ]\n",
        "    columns=[\"Pclass\", \"Sex\", \"Embarked\", \"SibSp\", \"Parch\", \"Fare\", \"Survived\"]\n",
        "  df = pd.concat(\n",
        "      data,\n",
        "      axis=1\n",
        "  )\n",
        "  return df.drop(columns=columns)\n",
        "def preprocess(df, test=False, base_preprocess=base_preprocess):\n",
        "  df = base_preprocess(df)\n",
        "  # test data have missing values in age and fare \n",
        "  if test:\n",
        "    df[\"Age\"] = df.Age.fillna(1) #1 is the mode of Age in training dataset\n",
        "    df[\"Fare\"] = df.Fare.fillna(0) #0 is the mode of Fare in training dataset\n",
        "  else: \n",
        "    df = sample(df)\n",
        "  df = cut_age(df)\n",
        "  df = cut_sibsp(df)\n",
        "  df = cut_parch(df)\n",
        "  df = cut_fare(df)\n",
        "  return df\n",
        "def age_preprocess(df, predict=False):\n",
        "  if not predict:\n",
        "    df = df[df.Age != -1]\n",
        "  df = hot_encoding(df, age=False)\n",
        "  return df\n",
        "def preprocess1(df):\n",
        "  df = df[df.Age != -1]\n",
        "  df = hot_encoding(df)\n",
        "  df = normalize_df(df)\n",
        "  return df\n",
        "def age_to_mode(x):\n",
        "  if x==-1: return 1\n",
        "  else: return x\n",
        "def preprocess2(df):\n",
        "  df = df.copy()\n",
        "  df[\"Age\"] = series_map(df.Age, age_to_mode)\n",
        "  df = hot_encoding(df)\n",
        "  df = normalize_df(df)\n",
        "  return df\n",
        "def preprocess3(df):\n",
        "  df = df.copy()\n",
        "  df[\"Age\"] = predicted_ages\n",
        "  df = hot_encoding(df)\n",
        "  df = normalize_df(df)\n",
        "  return df\n",
        "def only_remain_name_title(x):\n",
        "  return str(x.split()[1])\n",
        "def preprocess4(df):\n",
        "  df = df.copy()\n",
        "  df = df[df.Age != -1]\n",
        "  df[\"Name\"] = series_map(df.Name, only_remain_name_title)\n",
        "  for column in df.columns:\n",
        "    if column != \"Name\":\n",
        "      df[column] = df[column].astype(np.int64)\n",
        "  return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data preparation zone"
      ],
      "metadata": {
        "id": "_Bo6WM_kzsre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_base = preprocess(titanic_train)\n",
        "test_base = preprocess(titanic_test, test=True)\n",
        "\n",
        "train_base_2 = preprocess(titanic_train, base_preprocess=base_preprocess_2)\n",
        "test_base_2 = preprocess(titanic_test, test=True, base_preprocess=base_preprocess_2)"
      ],
      "metadata": {
        "id": "BIqwFudKzo41"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### delete NA age"
      ],
      "metadata": {
        "id": "Ik11RcSPPU5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_1 = preprocess1(train_base)\n",
        "full_test_1 = preprocess1(test_base)\n",
        "train_X_1, train_y_1, valid_X_1, valid_y_1 = split_train_valid(full_train_1, \"Survived\", X_columns=None, frac=0.85)\n",
        "full_train_X_1, full_train_y_1, _, _ = split_train_valid(full_train_1, \"Survived\", X_columns=None, frac=1)"
      ],
      "metadata": {
        "id": "XT2IqFCNPcrM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NA age replaced by mode of Age"
      ],
      "metadata": {
        "id": "MnBnL5klLDzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_2 = preprocess2(train_base)\n",
        "full_test_2 = preprocess2(test_base)\n",
        "train_X_2, train_y_2, valid_X_2, valid_y_2 = split_train_valid(full_train_2, \"Survived\", X_columns=None, frac=0.85)\n",
        "full_train_X_2, full_train_y_2, _, _ = split_train_valid(full_train_2, \"Survived\", X_columns=None, frac=1)"
      ],
      "metadata": {
        "id": "na2HT0Z2LI78"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NA age replaced by predicted values"
      ],
      "metadata": {
        "id": "hhUi7O6PWysm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_3 = preprocess3(train_base)\n",
        "full_test_3 = preprocess3(test_base)\n",
        "train_X_3, train_y_3, valid_X_3, valid_y_3 = split_train_valid(full_train_3, \"Survived\", X_columns=None, frac=0.85)\n",
        "full_train_X_3, full_train_y_3, _, _ = split_train_valid(full_train_3, \"Survived\", X_columns=None, frac=1)"
      ],
      "metadata": {
        "id": "IiyHDf7FWyJ-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Embedding"
      ],
      "metadata": {
        "id": "bRFZGx5aeWb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_4 = preprocess4(train_base_2)\n",
        "full_test_4 = preprocess4(test_base_2)\n",
        "train_X_4, train_y_4, valid_X_4, valid_y_4 = split_train_valid(full_train_4, \"Survived\", X_columns=None, frac=0.85)\n",
        "full_train_X_4, full_train_y_4, _, _ = split_train_valid(full_train_4, \"Survived\", X_columns=None, frac=1)\n",
        "train_X_4_tuple, valid_X_4_tuple, full_test_4_tuple = tuple(), tuple(), tuple()\n",
        "_full_test_4_ = full_test_4.to_numpy()\n",
        "for i in range(8):\n",
        "  try:\n",
        "    train_X_4_tuple += (tf.constant(train_X_4[:, i], dtype=tf.int64), )\n",
        "    valid_X_4_tuple += (tf.constant(valid_X_4[:, i], dtype=tf.int64), )\n",
        "    full_test_4_tuple += (tf.constant(_full_test_4_[:, i], dtype=tf.int64), )\n",
        "  except:\n",
        "    train_X_4_tuple += (tf.constant(train_X_4[:, i], dtype=tf.string), )\n",
        "    valid_X_4_tuple += (tf.constant(valid_X_4[:, i], dtype=tf.string), )\n",
        "    full_test_4_tuple += (tf.constant(_full_test_4_[:, i], dtype=tf.string), )"
      ],
      "metadata": {
        "id": "Z_r9jyb4eYnY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUg4Rf4YIAvv"
      },
      "source": [
        "## Algortithms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models"
      ],
      "metadata": {
        "id": "qyqd05di6PAq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wwTqXUmQha3"
      },
      "source": [
        "def deep_model(input_num, neurons):\n",
        "  keras.backend.clear_session()\n",
        "  np.random.seed(SEED)\n",
        "  tf.random.set_seed(SEED)\n",
        "  initial_inputs = keras.layers.Input(shape=input_num)\n",
        "  inputs = initial_inputs\n",
        "  for neuron in neurons:\n",
        "    inputs = dense_layer(neuron, inputs)\n",
        "  output = keras.layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
        "  model = keras.models.Model(\n",
        "    inputs=initial_inputs,\n",
        "    outputs=output\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def resnet_model(input_num, neurons):\n",
        "  # keep neurons number all same\n",
        "  keras.backend.clear_session()\n",
        "  np.random.seed(SEED)\n",
        "  tf.random.set_seed(SEED)\n",
        "  initial_inputs = keras.layers.Input(shape=input_num)\n",
        "  inputs = dense_layer(neurons[0], initial_inputs)\n",
        "  for neuron in neurons[1:]:\n",
        "    inputs += dense_layer(neuron, inputs)\n",
        "    inputs = keras.layers.BatchNormalization()(inputs)\n",
        "  output = keras.layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
        "  model = keras.models.Model(\n",
        "    inputs=initial_inputs,\n",
        "    outputs=output\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def densenet_model(input_num, neurons):\n",
        "  # keep neurons number all same\n",
        "  keras.backend.clear_session()\n",
        "  np.random.seed(SEED)\n",
        "  tf.random.set_seed(SEED)\n",
        "  initial_inputs = keras.layers.Input(shape=input_num) \n",
        "  concat_inputs = initial_inputs\n",
        "  for neuron in neurons:\n",
        "    prev_inputs = dense_layer(neuron, concat_inputs)\n",
        "    concat_inputs = keras.layers.concatenate(\n",
        "        [\n",
        "            prev_inputs,\n",
        "            concat_inputs\n",
        "        ]\n",
        "    )\n",
        "  output = keras.layers.Dense(1, activation=\"sigmoid\")(concat_inputs)\n",
        "  model = keras.models.Model(\n",
        "    inputs=initial_inputs,\n",
        "    outputs=output\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def resnet_and_wide_model(input_num, neurons):\n",
        "  # keep neurons number all same\n",
        "  keras.backend.clear_session()\n",
        "  np.random.seed(SEED)\n",
        "  tf.random.set_seed(SEED)\n",
        "  initial_inputs = keras.layers.Input(shape=input_num)\n",
        "  inputs = dense_layer(neurons[0], initial_inputs)\n",
        "  for neuron in neurons[1:]:\n",
        "    inputs += dense_layer(neuron, inputs)\n",
        "    inputs = keras.layers.BatchNormalization()(inputs)\n",
        "  final_inputs = keras.layers.concatenate(\n",
        "      [\n",
        "          inputs,\n",
        "          initial_inputs\n",
        "      ]\n",
        "  )\n",
        "  output = keras.layers.Dense(1, activation=\"sigmoid\")(final_inputs)\n",
        "  model = keras.models.Model(\n",
        "    inputs=initial_inputs,\n",
        "    outputs=output\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def deep_and_wide_model(input_num, neurons):\n",
        "  keras.backend.clear_session()\n",
        "  np.random.seed(SEED)\n",
        "  tf.random.set_seed(SEED)\n",
        "  initial_inputs = keras.layers.Input(shape=input_num)\n",
        "  inputs = initial_inputs\n",
        "  for neuron in neurons:\n",
        "    inputs = dense_layer(neuron, inputs)\n",
        "  final_inputs = keras.layers.concatenate(\n",
        "      [\n",
        "          inputs,\n",
        "          initial_inputs\n",
        "      ]\n",
        "  )\n",
        "  output = keras.layers.Dense(1, activation=\"sigmoid\")(final_inputs)\n",
        "  model = keras.models.Model(\n",
        "    inputs=initial_inputs,\n",
        "    outputs=output\n",
        "  )\n",
        "  return model\n",
        "\n",
        "\n",
        "def dcn_model(input_num, nerous):\n",
        "    keras.backend.clear_session()\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "    inputs = keras.layers.Input(shape=input_num)\n",
        "    c1 = tfrs.layers.dcn.Cross()(inputs, inputs)\n",
        "    c2 = tfrs.layers.dcn.Cross()(inputs, c1)\n",
        "    c3 = tfrs.layers.dcn.Cross()(inputs, c2)\n",
        "    c4 = tfrs.layers.dcn.Cross()(inputs, c3)\n",
        "    c5 = tfrs.layers.dcn.Cross()(inputs, c4)\n",
        "    c6 = tfrs.layers.dcn.Cross()(inputs, c5)\n",
        "    final_inputs = keras.layers.concatenate(\n",
        "        [\n",
        "            inputs,\n",
        "            c6\n",
        "        ]\n",
        "    )\n",
        "    for neuron in neurons:\n",
        "      final_inputs = dense_layer(neuron, final_inputs)\n",
        "    output = keras.layers.Dense(1, activation=\"sigmoid\")(final_inputs)\n",
        "    model = keras.models.Model(\n",
        "      inputs=inputs,\n",
        "      outputs=output\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def age_deep_model(input_num, neurons):\n",
        "  keras.backend.clear_session()\n",
        "  np.random.seed(SEED)\n",
        "  tf.random.set_seed(SEED)\n",
        "  initial_inputs = keras.layers.Input(shape=input_num)\n",
        "  inputs = initial_inputs\n",
        "  for neuron in neurons:\n",
        "    inputs = dense_layer(neuron, inputs)\n",
        "  output = keras.layers.Dense(3, activation=\"softmax\")(inputs)\n",
        "  model = keras.models.Model(\n",
        "    inputs=initial_inputs,\n",
        "    outputs=output\n",
        "  )\n",
        "  return model\n",
        "\n",
        "Pclass_vocab = train_base_2[\"Pclass\"].unique()\n",
        "Name_vocab = train_base_2[\"Name\"].unique()\n",
        "Sex_vocab = train_base_2[\"Sex\"].unique()\n",
        "Age_vocab = train_base_2[\"Age\"].unique()\n",
        "SibSp_vocab = train_base_2[\"SibSp\"].unique()\n",
        "Parch_vocab = train_base_2[\"Parch\"].unique()\n",
        "Fare_vocab = train_base_2[\"Fare\"].unique()\n",
        "Embarked_vocab = train_base_2[\"Embarked\"].unique()\n",
        "def deep_embedding_layer(neurons):\n",
        "  keras.backend.clear_session()\n",
        "  np.random.seed(SEED)\n",
        "  tf.random.set_seed(SEED)\n",
        "  pclass_input, pclass_embedding = embedding_layer(Pclass_vocab)\n",
        "  name_input, name_embedding = embedding_layer(Name_vocab, dtype=tf.string)\n",
        "  sex_input, sex_embedding = embedding_layer(Sex_vocab)\n",
        "  age_input, age_embedding = embedding_layer(Age_vocab)\n",
        "  sibsp_input, sibsp_embedding = embedding_layer(SibSp_vocab)\n",
        "  parch_input, parch_embedding = embedding_layer(Parch_vocab)\n",
        "  fare_input, fare_embedding = embedding_layer(Fare_vocab)\n",
        "  embarked_input, embarked_embedding = embedding_layer(Embarked_vocab)\n",
        "  inputs = keras.layers.concatenate(\n",
        "        [\n",
        "            pclass_embedding, \n",
        "            name_embedding, \n",
        "            sex_embedding, \n",
        "            age_embedding, \n",
        "            sibsp_embedding,\n",
        "            parch_embedding,\n",
        "            fare_embedding,\n",
        "            embarked_embedding\n",
        "        ]\n",
        "    )\n",
        "  for neuron in neurons:\n",
        "    inputs = dense_layer(neuron, inputs)\n",
        "  output = keras.layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
        "  embedding_inputs=[\n",
        "    pclass_input, \n",
        "    name_input, \n",
        "    sex_input, \n",
        "    age_input, \n",
        "    sibsp_input,\n",
        "    parch_input,\n",
        "    fare_input,\n",
        "    embarked_input\n",
        "  ]\n",
        "  model = keras.models.Model(\n",
        "    inputs=embedding_inputs,\n",
        "    outputs=output,\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loss function"
      ],
      "metadata": {
        "id": "6dQmK7oP9_QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_crossentropy = keras.losses.binary_crossentropy\n",
        "\n",
        "def dice_similarity_loss(y_true, y_pred):\n",
        "  dice_similarity = (2.0*tf.math.reduce_sum(y_true*y_pred)) / (tf.math.reduce_sum(y_true*y_true) + tf.math.reduce_sum(y_pred*y_pred))\n",
        "  return 1 - dice_similarity\n",
        "\n",
        "negative = sum(titanic_train.Survived==0)\n",
        "positive = sum(titanic_train.Survived==1)\n",
        "total = len(titanic_train)\n",
        "negative_weight = (1 / negative) * (total / 2.0)\n",
        "positive_weight = (1 / positive) * (total / 2.0)\n",
        "weight = {0: negative_weight, 1: positive_weight}\n",
        "print(weight)\n",
        "def weighted_loss(y_true, y_pred):\n",
        "  positive_true = y_true\n",
        "  negative_true = 1 - y_true\n",
        "  positive_pred = positive_true * y_pred\n",
        "  negative_pred = 1 - negative_true * y_pred\n",
        "  p = binary_crossentropy(positive_true, positive_pred)*positive_weight\n",
        "  n = binary_crossentropy(negative_true, negative_pred)*negative_weight\n",
        "  return p + n"
      ],
      "metadata": {
        "id": "RUqd3mWu-A8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "843bc194-1cda-431d-f79a-d9e22b8a6041"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.8114754098360656, 1: 1.3026315789473684}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Age Prediction Models"
      ],
      "metadata": {
        "id": "Y8X9KzR7yWBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# age_full_train = age_preprocess(train_base)\n",
        "# age_full_train_predict = age_preprocess(train_base, predict=True)\n",
        "\n",
        "# age_train_X, age_train_y, age_valid_X, age_valid_y = split_train_valid(age_full_train, \"Age\", X_columns=None, frac=0.85)\n",
        "\n",
        "# def age_by_model(x):\n",
        "#   if x.Age==-1:\n",
        "#     predictors = x[age_full_train.drop(columns=\"Age\").columns]\n",
        "#     predictors_to_np = np.array([predictors])\n",
        "#     predicted_ages = age_model.predict(predictors_to_np)\n",
        "#     return np.argmax(predicted_ages[0])\n",
        "#   return x.Age\n",
        "\n",
        "# def get_predicted_age(df):\n",
        "#   df = df.copy()\n",
        "#   return df.apply(age_by_model, axis=1)\n",
        "\n",
        "# neurons = [64, 64]\n",
        "# age_model = age_deep_model(len(age_train_X[0]), neurons) # (13, 0.4490162432193756, 0.7678571343421936, 0.8317757248878479)\n",
        "# hist = train_model(\n",
        "#     age_model, age_train_X, age_train_y, age_valid_X, age_valid_y, \n",
        "#     keras.losses.sparse_categorical_crossentropy,\n",
        "#     metrics=[],\n",
        "#     monitor=\"val_loss\", class_weight={0: 10, 1: 0.55, 2: 0.55})\n",
        "# predicted_ages = get_predicted_age(age_full_train_predict)"
      ],
      "metadata": {
        "id": "XqCFyvK0VpXz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th8s8T4jIH22"
      },
      "source": [
        "### Train Survived Prediction Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUjAn1MbS-tj"
      },
      "source": [
        "# neurons = [64, 64]\n",
        "# deep_model_based_on_preprocess1_bc = deep_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(deep_model_based_on_preprocess1_bc, train_X_1, train_y_1, valid_X_1, valid_y_1, binary_crossentropy)\n",
        "\n",
        "# neurons = [64, 64]\n",
        "# deep_model_based_on_preprocess1_dsl = deep_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(deep_model_based_on_preprocess1_dsl, train_X_1, train_y_1, valid_X_1, valid_y_1, dice_similarity_loss)\n",
        "\n",
        "# neurons = [64, 64]\n",
        "# deep_model_based_on_preprocess1_wbc = deep_model(len(train_X_1[0]), neurons)\n",
        "# hist = train_model(deep_model_based_on_preprocess1_wbc, train_X_1, train_y_1, valid_X_1, valid_y_1, weighted_loss)\n",
        "\n",
        "# neurons = [64, 64]\n",
        "# deep_and_wide_model_based_on_preprocess1_bc = deep_and_wide_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(deep_and_wide_model_based_on_preprocess1_bc, train_X_1, train_y_1, valid_X_1, valid_y_1, binary_crossentropy)\n",
        "\n",
        "# neurons = [64, 64]\n",
        "# deep_and_wide_model_based_on_preprocess1_dsl = deep_and_wide_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(deep_and_wide_model_based_on_preprocess1_dsl, train_X_1, train_y_1, valid_X_1, valid_y_1, dice_similarity_loss)\n",
        "\n",
        "# neurons = [64, 64]\n",
        "# deep_and_wide_model_based_on_preprocess1_wbc = deep_and_wide_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(deep_and_wide_model_based_on_preprocess1_wbc, train_X_1, train_y_1, valid_X_1, valid_y_1, weighted_loss)\n",
        "\n",
        "# neurons = [64, 64]\n",
        "# dcn_model_based_on_preprocess1_bc = dcn_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(dcn_model_based_on_preprocess1_bc, train_X_1, train_y_1, valid_X_1, valid_y_1, binary_crossentropy)\n",
        "\n",
        "# neurons = [64, 64]\n",
        "# dcn_model_based_on_preprocess1_dsl = dcn_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(dcn_model_based_on_preprocess1_dsl, train_X_1, train_y_1, valid_X_1, valid_y_1, dice_similarity_loss)\n",
        "\n",
        "# neurons = [64, 64]\n",
        "# dcn_model_based_on_preprocess1_wbc = dcn_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(dcn_model_based_on_preprocess1_wbc, train_X_1, train_y_1, valid_X_1, valid_y_1, weighted_loss)\n",
        "\n",
        "# neurons = [64] * 4\n",
        "# resnet_model_based_on_preprocess1_bc = resnet_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(resnet_model_based_on_preprocess1_bc, train_X_1, train_y_1, valid_X_1, valid_y_1, binary_crossentropy)\n",
        "\n",
        "# neurons = [64] * 4\n",
        "# resnet_and_wide_model_based_on_preprocess1_bc = resnet_and_wide_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(resnet_and_wide_model_based_on_preprocess1_bc, train_X_1, train_y_1, valid_X_1, valid_y_1, binary_crossentropy)\n",
        "\n",
        "# neurons = [64] * 4\n",
        "# densenet_model_based_on_preprocess1_bc = densenet_model(len(train_X_1[0]), neurons) \n",
        "# hist = train_model(densenet_model_based_on_preprocess1_bc, train_X_1, train_y_1, valid_X_1, valid_y_1, binary_crossentropy)\n",
        "\n",
        "# rfc = RandomForestClassifier(max_depth=5, n_estimators=100, criterion=\"gini\", random_state=SEED) \n",
        "# rfc.fit(train_X_1, train_y_1)\n",
        "\n",
        "# naive_bayes_classifier = ComplementNB() \n",
        "# naive_bayes_classifier.fit(train_X_1, train_y_1)\n",
        "\n",
        "# neigh = KNeighborsClassifier(n_neighbors=20)\n",
        "# neigh.fit(train_X_1, train_y_1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rfc = RandomForestClassifier(max_depth=5, n_estimators=100, criterion=\"gini\", random_state=SEED) \n",
        "# rfc.fit(train_X_1, train_y_1)\n",
        "# neigh = KNeighborsClassifier(n_neighbors=20)\n",
        "# neigh.fit(train_X_1, train_y_1)\n",
        "# print(rfc.score(valid_X_1, valid_y_1))\n",
        "# print(neigh.score(valid_X_1, valid_y_1))\n",
        "\n",
        "# m = get_model_without_top(deep_model_based_on_preprocess1_bc)\n",
        "# train_X_1_output = m(train_X_1)\n",
        "# valid_X_1_output = m(valid_X_1)\n",
        "# rfc = RandomForestClassifier(max_depth=5, n_estimators=100, criterion=\"gini\", random_state=SEED) \n",
        "# rfc.fit(train_X_1_output, train_y_1)\n",
        "# neigh = KNeighborsClassifier(n_neighbors=20)\n",
        "# neigh.fit(train_X_1_output, train_y_1)\n",
        "# print(rfc.score(valid_X_1_output, valid_y_1))\n",
        "# print(neigh.score(valid_X_1_output, valid_y_1))"
      ],
      "metadata": {
        "id": "zwbyTmV5LJyh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neurons = [64, 64]\n",
        "# deep_model_based_on_preprocess2_bc = deep_model(len(train_X_2[0]), neurons) \n",
        "# hist = train_model(deep_model_based_on_preprocess2_bc, train_X_2, train_y_2, valid_X_2, valid_y_2, binary_crossentropy)\n",
        "\n",
        "# deep_and_wide_model_based_on_preprocess2_bc = deep_and_wide_model(len(train_X_2[0]), neurons) \n",
        "# hist = train_model(deep_and_wide_model_based_on_preprocess2_bc, train_X_2, train_y_2, valid_X_2, valid_y_2, binary_crossentropy)\n",
        "\n",
        "# dcn_model_based_on_preprocess2_bc = dcn_model(len(train_X_2[0]), neurons) \n",
        "# hist = train_model(dcn_model_based_on_preprocess2_bc, train_X_2, train_y_2, valid_X_2, valid_y_2, binary_crossentropy)"
      ],
      "metadata": {
        "id": "bW6xMWCKIHPh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neurons = [64, 64]\n",
        "# deep_model_based_on_preprocess3_bc = deep_model(len(train_X_3[0]), neurons) \n",
        "# hist = train_model(deep_model_based_on_preprocess3_bc, train_X_3, train_y_3, valid_X_3, valid_y_3, binary_crossentropy)\n",
        "\n",
        "# deep_and_wide_model_based_on_preprocess3_bc = deep_and_wide_model(len(train_X_3[0]), neurons) \n",
        "# hist = train_model(deep_and_wide_model_based_on_preprocess3_bc, train_X_3, train_y_3, valid_X_3, valid_y_3, binary_crossentropy)\n",
        "\n",
        "# dcn_model_based_on_preprocess3_bc = dcn_model(len(train_X_3[0]), neurons) \n",
        "# hist = train_model(dcn_model_based_on_preprocess3_bc, train_X_3, train_y_3, valid_X_3, valid_y_3, binary_crossentropy)"
      ],
      "metadata": {
        "id": "zWzVVSfjXA1C"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neurons = [64, 64]\n",
        "\n",
        "# deep_embedding_model_based_on_preprocess4_bc = deep_embedding_layer(neurons) \n",
        "# hist = train_model(deep_embedding_model_based_on_preprocess4_bc, train_X_4_tuple, train_y_4, valid_X_4_tuple, valid_y_4, binary_crossentropy)"
      ],
      "metadata": {
        "id": "iZVMihooWdek"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc8_CTUHN9Oi"
      },
      "source": [
        "## submit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfFyCSfGleNF"
      },
      "source": [
        "class _rfc_:\n",
        "  def predict(X):\n",
        "    return rfc.predict_proba(X)[:, 1]\n",
        "class _knn_:\n",
        "  def predict(X):\n",
        "    return neigh.predict_proba(X)[:, 1]\n",
        "class _naive_bayes_classifier_:\n",
        "  def predict(X):\n",
        "    return naive_bayes_classifier.predict_proba(X)[:, 1]\n",
        "    \n",
        "predictions = ensemble_submit( \n",
        "     [(deep_model_based_on_preprocess1_bc, full_test_1, 1.0316705), \n",
        "     (deep_model_based_on_preprocess1_dsl, full_test_1, 1.0291687),\n",
        "     (deep_model_based_on_preprocess1_wbc, full_test_1, 1.0302162), \n",
        "     (deep_and_wide_model_based_on_preprocess1_bc, full_test_1, 1.0319707),\n",
        "     (deep_and_wide_model_based_on_preprocess1_dsl, full_test_1, 1.0209073), \n",
        "     (deep_and_wide_model_based_on_preprocess1_wbc, full_test_1, 1.0153959),\n",
        "     (dcn_model_based_on_preprocess1_bc, full_test_1, 1.0327203),\n",
        "     (dcn_model_based_on_preprocess1_dsl, full_test_1, 1.0107774), \n",
        "     (dcn_model_based_on_preprocess1_wbc, full_test_1, 1.0322365),\n",
        "     (resnet_model_based_on_preprocess1_bc, full_test_1, 1.0200506),\n",
        "     (resnet_and_wide_model_based_on_preprocess1_bc, full_test_1, 1.032436),\n",
        "     (densenet_model_based_on_preprocess1_bc, full_test_1, 1.0307261),\n",
        "     (rfc, full_test_1, 2), \n",
        "     (neigh, full_test_1, 1), \n",
        "     (naive_bayes_classifier, full_test_1, 1),\n",
        "     (deep_model_based_on_preprocess2_bc, full_test_2, 0.96794057), \n",
        "     (deep_and_wide_model_based_on_preprocess2_bc, full_test_2, 0.9678019),\n",
        "     (dcn_model_based_on_preprocess2_bc, full_test_2, 0.9684922),\n",
        "     (deep_model_based_on_preprocess3_bc, full_test_3, 0.9678136), \n",
        "     (deep_and_wide_model_based_on_preprocess3_bc, full_test_3, 0.96767807),\n",
        "     (dcn_model_based_on_preprocess3_bc, full_test_3, 0.9682598),\n",
        "     (deep_embedding_model_based_on_preprocess4_bc, full_test_4_tuple, 0.9899622), \n",
        "     ]\n",
        ")"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models_for_output_test_predictions = [\n",
        "# (deep_model_based_on_preprocess1_bc, full_test_1), \n",
        "# (deep_model_based_on_preprocess1_dsl, full_test_1),\n",
        "# (deep_model_based_on_preprocess1_wbc, full_test_1), \n",
        "# (deep_and_wide_model_based_on_preprocess1_bc, full_test_1),\n",
        "# (deep_and_wide_model_based_on_preprocess1_dsl, full_test_1), \n",
        "# (deep_and_wide_model_based_on_preprocess1_wbc, full_test_1),\n",
        "# (dcn_model_based_on_preprocess1_bc, full_test_1),\n",
        "# (dcn_model_based_on_preprocess1_dsl, full_test_1), \n",
        "# (dcn_model_based_on_preprocess1_wbc, full_test_1),\n",
        "# (resnet_model_based_on_preprocess1_bc, full_test_1),\n",
        "# (resnet_and_wide_model_based_on_preprocess1_bc, full_test_1),\n",
        "# (densenet_model_based_on_preprocess1_bc, full_test_1),\n",
        "# (rfc, full_test_1), \n",
        "# (neigh, full_test_1), \n",
        "# (naive_bayes_classifier, full_test_1),\n",
        "# (deep_model_based_on_preprocess2_bc, full_test_2), \n",
        "# (deep_and_wide_model_based_on_preprocess2_bc, full_test_2),\n",
        "# (dcn_model_based_on_preprocess2_bc, full_test_2),\n",
        "# (deep_model_based_on_preprocess3_bc, full_test_3[0:107]), \n",
        "# (deep_and_wide_model_based_on_preprocess3_bc, full_test_3),\n",
        "# (dcn_model_based_on_preprocess3_bc, full_test_3),\n",
        "# (deep_embedding_model_based_on_preprocess4_bc, full_test_4_tuple), \n",
        "# ]\n",
        "\n",
        "# models_for_output_train_predictions = [\n",
        "# (deep_model_based_on_preprocess1_bc, train_X_1), \n",
        "# (deep_model_based_on_preprocess1_dsl, train_X_1),\n",
        "# (deep_model_based_on_preprocess1_wbc, train_X_1), \n",
        "# (deep_and_wide_model_based_on_preprocess1_bc, train_X_1),\n",
        "# (deep_and_wide_model_based_on_preprocess1_dsl, train_X_1), \n",
        "# (deep_and_wide_model_based_on_preprocess1_wbc, train_X_1),\n",
        "# (dcn_model_based_on_preprocess1_bc, train_X_1),\n",
        "# (dcn_model_based_on_preprocess1_dsl, train_X_1), \n",
        "# (dcn_model_based_on_preprocess1_wbc, train_X_1),\n",
        "# (resnet_model_based_on_preprocess1_bc, train_X_1),\n",
        "# (resnet_and_wide_model_based_on_preprocess1_bc, train_X_1),\n",
        "# (densenet_model_based_on_preprocess1_bc, train_X_1),\n",
        "# (rfc, train_X_1), \n",
        "# (neigh, train_X_1), \n",
        "# (naive_bayes_classifier, train_X_1),\n",
        "# (deep_model_based_on_preprocess2_bc, train_X_2[0:605]), \n",
        "# (deep_and_wide_model_based_on_preprocess2_bc, train_X_2[0:605]),\n",
        "# (dcn_model_based_on_preprocess2_bc, train_X_2[0:605]),\n",
        "# (deep_model_based_on_preprocess3_bc, train_X_3[0:605]), \n",
        "# (deep_and_wide_model_based_on_preprocess3_bc, train_X_3[0:605]),\n",
        "# (dcn_model_based_on_preprocess3_bc, train_X_3[0:605]),\n",
        "# (deep_embedding_model_based_on_preprocess4_bc, train_X_4_tuple), \n",
        "# ]\n",
        "\n",
        "# models_for_output_valid_predictions = [\n",
        "# (deep_model_based_on_preprocess1_bc, valid_X_1), \n",
        "# (deep_model_based_on_preprocess1_dsl, valid_X_1),\n",
        "# (deep_model_based_on_preprocess1_wbc, valid_X_1), \n",
        "# (deep_and_wide_model_based_on_preprocess1_bc, valid_X_1),\n",
        "# (deep_and_wide_model_based_on_preprocess1_dsl, valid_X_1), \n",
        "# (deep_and_wide_model_based_on_preprocess1_wbc, valid_X_1),\n",
        "# (dcn_model_based_on_preprocess1_bc, valid_X_1),\n",
        "# (dcn_model_based_on_preprocess1_dsl, valid_X_1), \n",
        "# (dcn_model_based_on_preprocess1_wbc, valid_X_1),\n",
        "# (resnet_model_based_on_preprocess1_bc, valid_X_1),\n",
        "# (resnet_and_wide_model_based_on_preprocess1_bc, valid_X_1),\n",
        "# (densenet_model_based_on_preprocess1_bc, valid_X_1),\n",
        "# (rfc, valid_X_1), \n",
        "# (neigh, valid_X_1), \n",
        "# (naive_bayes_classifier, valid_X_1),\n",
        "# (deep_model_based_on_preprocess2_bc, valid_X_2[0:107]), \n",
        "# (deep_and_wide_model_based_on_preprocess2_bc, valid_X_2[0:107]),\n",
        "# (dcn_model_based_on_preprocess2_bc, valid_X_2[0:107]),\n",
        "# (deep_model_based_on_preprocess3_bc, valid_X_3[0:107]), \n",
        "# (deep_and_wide_model_based_on_preprocess3_bc, valid_X_3[0:107]),\n",
        "# (dcn_model_based_on_preprocess3_bc, valid_X_3[0:107]),\n",
        "# (deep_embedding_model_based_on_preprocess4_bc, valid_X_4_tuple), \n",
        "# ]\n",
        "\n",
        "# input_test_tuples = tuple()\n",
        "# input_train_tuples = tuple()\n",
        "# input_valid_tuples = tuple()\n",
        "\n",
        "# def get_inputs_for_ensemble(models_for_predictions, t):\n",
        "#   for m, d in models_for_predictions:\n",
        "#     t += (np.reshape((m.predict(d)), (-1,1)), )\n",
        "#   return t\n",
        "\n",
        "# test_predictions = get_inputs_for_ensemble(models_for_output_test_predictions, input_test_tuples)\n",
        "# train_predictions = get_inputs_for_ensemble(models_for_output_train_predictions, input_train_tuples)\n",
        "# valid_predictions = get_inputs_for_ensemble(models_for_output_valid_predictions, input_valid_tuples)\n",
        "\n",
        "# class _1d(keras.layers.Layer):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         super().__init__(**kwargs)\n",
        "#     def build(self, batch_input_shape):\n",
        "#         self.kernel = self.add_weight(\n",
        "#             name=\"kernel\", shape=[22],\n",
        "#             initializer=keras.initializers.constant(1))\n",
        "#         super().build(batch_input_shape) \n",
        "#     def call(self, X):\n",
        "#         return tf.tensordot(X, self.kernel, 1) / tf.math.reduce_sum(self.kernel)\n",
        "                                \n",
        "# def ensemble_models():\n",
        "#   in_inputs = []\n",
        "#   for i in tf.range(22):\n",
        "#     in_inputs.append(keras.layers.Input(shape=(1,)))\n",
        "#   c = keras.layers.concatenate(in_inputs)\n",
        "#   outputs = _1d()(c)\n",
        "#   model = keras.models.Model(\n",
        "#     inputs=in_inputs,\n",
        "#     outputs=outputs,\n",
        "#   )\n",
        "#   return model\n",
        "\n",
        "# m = ensemble_models()\n",
        "\n",
        "# hist = train_model(m, train_predictions, train_y_1, valid_predictions, valid_y_1, \n",
        "#                    binary_crossentropy,\n",
        "#                    patience=20)"
      ],
      "metadata": {
        "id": "XWdRYBsj999P"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jpsUbceMFL_",
        "outputId": "f8649e60-ed4b-416d-80b3-6616da382cba"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'private_1d_23/kernel:0' shape=(22,) dtype=float32, numpy=\n",
              " array([1.0316705 , 1.0291687 , 1.0302162 , 1.0319707 , 1.0209073 ,\n",
              "        1.0153959 , 1.0327203 , 1.0107774 , 1.0322365 , 1.0200506 ,\n",
              "        1.032436  , 1.0307261 , 1.0315827 , 1.027979  , 1.0192858 ,\n",
              "        0.96794057, 0.9678019 , 0.9684922 , 0.9678136 , 0.96767807,\n",
              "        0.9682598 , 0.9899622 ], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# submit(test_predictions, m)"
      ],
      "metadata": {
        "id": "Ilrnp-tbMBEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9NVF-jmh0qF",
        "outputId": "c1f3320c-8766-4353-8ce9-5de1e9e49112"
      },
      "source": [
        "# !kaggle competitions submit -c titanic -f /content/submission_df -m \"knn rfc bayes dl ensemble with learning rates\"\n",
        "# !kaggle competitions submit -c titanic -f /content/submission_df -m \"deep_embedding_model_based_on_preprocess4_bc\""
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 2.77k/2.77k [00:00<00:00, 7.74kB/s]\n",
            "Successfully submitted to Titanic - Machine Learning from Disaster"
          ]
        }
      ]
    }
  ]
}